<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Margin-based Losses · LossFunctions.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaML.github.io/LossFunctions.jl/losses/margin/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LossFunctions.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="tocitem" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="tocitem">User&#39;s Guide</span><ul><li><a class="tocitem" href="../../user/interface/">Working with Losses</a></li><li><a class="tocitem" href="../../user/aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="tocitem">Available Losses</span><ul><li><a class="tocitem" href="../distance/">Distance-based Losses</a></li><li class="is-active"><a class="tocitem" href>Margin-based Losses</a><ul class="internal"><li><a class="tocitem" href="#ZeroOneLoss"><span>ZeroOneLoss</span></a></li><li><a class="tocitem" href="#PerceptronLoss"><span>PerceptronLoss</span></a></li><li><a class="tocitem" href="#L1HingeLoss"><span>L1HingeLoss</span></a></li><li><a class="tocitem" href="#SmoothedL1HingeLoss"><span>SmoothedL1HingeLoss</span></a></li><li><a class="tocitem" href="#ModifiedHuberLoss"><span>ModifiedHuberLoss</span></a></li><li><a class="tocitem" href="#DWDMarginLoss"><span>DWDMarginLoss</span></a></li><li><a class="tocitem" href="#L2MarginLoss"><span>L2MarginLoss</span></a></li><li><a class="tocitem" href="#L2HingeLoss"><span>L2HingeLoss</span></a></li><li><a class="tocitem" href="#LogitMarginLoss"><span>LogitMarginLoss</span></a></li><li><a class="tocitem" href="#ExpLoss"><span>ExpLoss</span></a></li><li><a class="tocitem" href="#SigmoidLoss"><span>SigmoidLoss</span></a></li></ul></li><li><a class="tocitem" href="../other/">Other Losses</a></li></ul></li><li><span class="tocitem">Advances Topics</span><ul><li><a class="tocitem" href="../../advanced/extend/">Altering existing Losses</a></li><li><a class="tocitem" href="../../advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="tocitem" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="tocitem" href="../../LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Available Losses</a></li><li class="is-active"><a href>Margin-based Losses</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Margin-based Losses</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaML/LossFunctions.jl/blob/master/docs/src/losses/margin.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Margin-based-Losses"><a class="docs-heading-anchor" href="#Margin-based-Losses">Margin-based Losses</a><a id="Margin-based-Losses-1"></a><a class="docs-heading-anchor-permalink" href="#Margin-based-Losses" title="Permalink"></a></h1><p>Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction. Instead they penalize predictions based on how well they agree with the sign of the target.</p><p>This section lists all the subtypes of <a href="losses/@ref"><code>MarginLoss</code></a> that are implemented in this package.</p><h2 id="ZeroOneLoss"><a class="docs-heading-anchor" href="#ZeroOneLoss">ZeroOneLoss</a><a id="ZeroOneLoss-1"></a><a class="docs-heading-anchor-permalink" href="#ZeroOneLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.ZeroOneLoss" href="#LossFunctions.ZeroOneLoss"><code>LossFunctions.ZeroOneLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZeroOneLoss &lt;: MarginLoss</code></pre><p>The classical classification loss. It penalizes every misclassified observation with a loss of <code>1</code> while every correctly classified observation has a loss of <code>0</code>. It is not convex nor continuous and thus seldom used directly. Instead one usually works with some classification-calibrated surrogate loss, such as <a href="#L1HingeLoss">L1HingeLoss</a>.</p><p class="math-container">\[L(a) = \begin{cases} 1 &amp; \quad \text{if } a &lt; 0 \\ 0 &amp; \quad \text{if } a &gt;= 0\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    1 │------------┐            │    1 │                         │
      │            |            │      │                         │
      │            |            │      │                         │
      │            |            │      │_________________________│
      │            |            │      │                         │
      │            |            │      │                         │
      │            |            │      │                         │
    0 │            └------------│   -1 │                         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                y * h(x)                         y * h(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L5">source</a></section></article><h2 id="PerceptronLoss"><a class="docs-heading-anchor" href="#PerceptronLoss">PerceptronLoss</a><a id="PerceptronLoss-1"></a><a class="docs-heading-anchor-permalink" href="#PerceptronLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.PerceptronLoss" href="#LossFunctions.PerceptronLoss"><code>LossFunctions.PerceptronLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerceptronLoss &lt;: MarginLoss</code></pre><p>The perceptron loss linearly penalizes every prediction where the resulting <code>agreement &lt;= 0</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \max \{ 0, -a \}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\.                       │    0 │            ┌------------│
      │ &#39;..                     │      │            |            │
      │   \.                    │      │            |            │
      │     &#39;.                  │      │            |            │
    L │      &#39;.                 │   L&#39; │            |            │
      │        \.               │      │            |            │
      │         &#39;.              │      │            |            │
    0 │           \.____________│   -1 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L58">source</a></section></article><h2 id="L1HingeLoss"><a class="docs-heading-anchor" href="#L1HingeLoss">L1HingeLoss</a><a id="L1HingeLoss-1"></a><a class="docs-heading-anchor-permalink" href="#L1HingeLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L1HingeLoss" href="#LossFunctions.L1HingeLoss"><code>LossFunctions.L1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L1HingeLoss &lt;: MarginLoss</code></pre><p>The hinge loss linearly penalizes every predicition where the resulting <code>agreement &lt; 1</code> . It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \max \{ 0, 1 - a \}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    3 │&#39;\.                      │    0 │                  ┌------│
      │  &#39;&#39;_                    │      │                  |      │
      │     \.                  │      │                  |      │
      │       &#39;.                │      │                  |      │
    L │         &#39;&#39;_             │   L&#39; │                  |      │
      │            \.           │      │                  |      │
      │              &#39;.         │      │                  |      │
    0 │                &#39;&#39;_______│   -1 │------------------┘      │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L150">source</a></section></article><h2 id="SmoothedL1HingeLoss"><a class="docs-heading-anchor" href="#SmoothedL1HingeLoss">SmoothedL1HingeLoss</a><a id="SmoothedL1HingeLoss-1"></a><a class="docs-heading-anchor-permalink" href="#SmoothedL1HingeLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.SmoothedL1HingeLoss" href="#LossFunctions.SmoothedL1HingeLoss"><code>LossFunctions.SmoothedL1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SmoothedL1HingeLoss &lt;: MarginLoss</code></pre><p>As the name suggests a smoothed version of the L1 hinge loss. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \begin{cases} \frac{0.5}{\gamma} \cdot \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge 1 - \gamma \\ 1 - \frac{\gamma}{2} - a &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (γ=2)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\.                       │    0 │                 ,r------│
      │ &#39;.                      │      │               ./&#39;       │
      │   \.                    │      │              ,/         │
      │     &#39;.                  │      │            ./&#39;          │
    L │      &#39;.                 │   L&#39; │           ,&#39;            │
      │        \.               │      │         ,/              │
      │          &#39;,             │      │       ./&#39;               │
    0 │            &#39;*-._________│   -1 │______./                 │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L248">source</a></section></article><h2 id="ModifiedHuberLoss"><a class="docs-heading-anchor" href="#ModifiedHuberLoss">ModifiedHuberLoss</a><a id="ModifiedHuberLoss-1"></a><a class="docs-heading-anchor-permalink" href="#ModifiedHuberLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.ModifiedHuberLoss" href="#LossFunctions.ModifiedHuberLoss"><code>LossFunctions.ModifiedHuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ModifiedHuberLoss &lt;: MarginLoss</code></pre><p>A special (4 times scaled) case of the <a href="#LossFunctions.SmoothedL1HingeLoss"><code>SmoothedL1HingeLoss</code></a> with <code>γ=2</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \begin{cases} \max \{ 0, 1 - a \} ^2 &amp; \quad \text{if } a \ge -1 \\ - 4 a &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │    &#39;.                   │    0 │                .+-------│
      │     &#39;.                  │      │              ./&#39;        │
      │      &#39;\                 │      │             ,/          │
      │        \                │      │           ,/            │
    L │         &#39;.              │   L&#39; │         ./              │
      │          &#39;.             │      │       ./&#39;               │
      │            \.           │      │______/&#39;                 │
    0 │              &#39;-.________│   -5 │                         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L317">source</a></section></article><h2 id="DWDMarginLoss"><a class="docs-heading-anchor" href="#DWDMarginLoss">DWDMarginLoss</a><a id="DWDMarginLoss-1"></a><a class="docs-heading-anchor-permalink" href="#DWDMarginLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.DWDMarginLoss" href="#LossFunctions.DWDMarginLoss"><code>LossFunctions.DWDMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DWDMarginLoss &lt;: MarginLoss</code></pre><p>The distance weighted discrimination margin loss. It is a differentiable generalization of the <a href="#L1HingeLoss">L1HingeLoss</a> that is different than the <a href="#SmoothedL1HingeLoss">SmoothedL1HingeLoss</a>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \begin{cases} 1 - a &amp; \quad \text{if } a \ge \frac{q}{q+1} \\ \frac{1}{a^q} \frac{q^q}{(q+1)^{q+1}} &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (q=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │      &quot;.                 │    0 │                     ._r-│
      │        \.               │      │                   ./    │
      │         &#39;,              │      │                 ./      │
      │           \.            │      │                 /       │
    L │            &quot;\.          │   L&#39; │                .        │
      │              \.         │      │                /        │
      │               &quot;:__      │      │               ;         │
    0 │                   &#39;&quot;&quot;---│   -1 │---------------┘         │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L520">source</a></section></article><h2 id="L2MarginLoss"><a class="docs-heading-anchor" href="#L2MarginLoss">L2MarginLoss</a><a id="L2MarginLoss-1"></a><a class="docs-heading-anchor-permalink" href="#L2MarginLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L2MarginLoss" href="#LossFunctions.L2MarginLoss"><code>LossFunctions.L2MarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2MarginLoss &lt;: MarginLoss</code></pre><p>The margin-based least-squares loss for classification, which penalizes every prediction where <code>agreement != 1</code> quadratically. It is locally Lipschitz continuous and strongly convex.</p><p class="math-container">\[L(a) = {\left( 1 - a \right)}^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │     .                   │    2 │                       ,r│
      │     &#39;.                  │      │                     ,/  │
      │      &#39;\                 │      │                   ,/    │
      │        \                │      ├                 ,/      ┤
    L │         &#39;.              │   L&#39; │               ./        │
      │          &#39;.             │      │             ./          │
      │            \.          .│      │           ./            │
    0 │              &#39;-.____.-&#39; │   -3 │         ./              │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L374">source</a></section></article><h2 id="L2HingeLoss"><a class="docs-heading-anchor" href="#L2HingeLoss">L2HingeLoss</a><a id="L2HingeLoss-1"></a><a class="docs-heading-anchor-permalink" href="#L2HingeLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L2HingeLoss" href="#LossFunctions.L2HingeLoss"><code>LossFunctions.L2HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2HingeLoss &lt;: MarginLoss</code></pre><p>The truncated least squares loss quadratically penalizes every predicition where the resulting <code>agreement &lt; 1</code>. It is locally Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(a) = \max \{ 0, 1 - a \}^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │     .                   │    0 │                 ,r------│
      │     &#39;.                  │      │               ,/        │
      │      &#39;\                 │      │             ,/          │
      │        \                │      │           ,/            │
    L │         &#39;.              │   L&#39; │         ./              │
      │          &#39;.             │      │       ./                │
      │            \.           │      │     ./                  │
    0 │              &#39;-.________│   -5 │   ./                    │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L198">source</a></section></article><h2 id="LogitMarginLoss"><a class="docs-heading-anchor" href="#LogitMarginLoss">LogitMarginLoss</a><a id="LogitMarginLoss-1"></a><a class="docs-heading-anchor-permalink" href="#LogitMarginLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.LogitMarginLoss" href="#LossFunctions.LogitMarginLoss"><code>LossFunctions.LogitMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogitMarginLoss &lt;: MarginLoss</code></pre><p>The margin version of the logistic loss. It is infinitely many times differentiable, strictly convex, and Lipschitz continuous.</p><p class="math-container">\[L(a) = \ln (1 + e^{-a})\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │ \.                      │    0 │                  ._--/&quot;&quot;│
      │   \.                    │      │               ../&#39;      │
      │     \.                  │      │              ./         │
      │       \..               │      │            ./&#39;          │
    L │         &#39;-_             │   L&#39; │          .,&#39;            │
      │            &#39;-_          │      │         ./              │
      │               &#39;\-._     │      │      .,/&#39;               │
    0 │                    &#39;&quot;&quot;*-│   -1 │__.--&#39;&#39;                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -4                        4
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L104">source</a></section></article><h2 id="ExpLoss"><a class="docs-heading-anchor" href="#ExpLoss">ExpLoss</a><a id="ExpLoss-1"></a><a class="docs-heading-anchor-permalink" href="#ExpLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.ExpLoss" href="#LossFunctions.ExpLoss"><code>LossFunctions.ExpLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ExpLoss &lt;: MarginLoss</code></pre><p>The margin-based exponential loss for classification, which penalizes every prediction exponentially. It is infinitely many times differentiable, locally Lipschitz continuous and strictly convex, but not clipable.</p><p class="math-container">\[L(a) = e^{-a}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    5 │  \.                     │    0 │               _,,---:&#39;&quot;&quot;│
      │   l                     │      │           _r/&quot;&#39;         │
      │    l.                   │      │        .r/&#39;             │
      │     &quot;:                  │      │      .r&#39;                │
    L │       \.                │   L&#39; │     ./                  │
      │        &quot;\..             │      │    .&#39;                   │
      │           &#39;&quot;:,_         │      │   ,&#39;                    │
    0 │                &quot;&quot;---:.__│   -5 │  ./                     │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L422">source</a></section></article><h2 id="SigmoidLoss"><a class="docs-heading-anchor" href="#SigmoidLoss">SigmoidLoss</a><a id="SigmoidLoss-1"></a><a class="docs-heading-anchor-permalink" href="#SigmoidLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.SigmoidLoss" href="#LossFunctions.SigmoidLoss"><code>LossFunctions.SigmoidLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SigmoidLoss &lt;: MarginLoss</code></pre><p>Continuous loss which penalizes every prediction with a loss within in the range (0,2). It is infinitely many times differentiable, Lipschitz continuous but nonconvex.</p><p class="math-container">\[L(a) = 1 - \tanh(a)\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │&quot;&quot;&#39;--,.                  │    0 │..                     ..│
      │      &#39;\.                │      │ &quot;\.                 ./&quot; │
      │         &#39;.              │      │    &#39;,             ,&#39;    │
      │           \.            │      │      \           /      │
    L │            &quot;\.          │   L&#39; │       \         /       │
      │              \.         │      │        \.     ./        │
      │                \,       │      │         \.   ./         │
    0 │                  &#39;&quot;-:.__│   -1 │          &#39;,_,&#39;          │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 y ⋅ ŷ                            y ⋅ ŷ</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/0b411a14601d19c44d735e5fce0185023e259bb2/src/losses/margin.jl#L471">source</a></section></article></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../distance/">« Distance-based Losses</a><a class="docs-footer-nextpage" href="../other/">Other Losses »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 24 August 2023 11:15">Thursday 24 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
