<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · LossFunctions.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://JuliaML.github.io/LossFunctions.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="assets/style.css" rel="stylesheet" type="text/css"/><link href="assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.png" alt="LossFunctions.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>LossFunctions.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Introduction-and-Motivation"><span>Introduction and Motivation</span></a></li><li><a class="tocitem" href="#User&#39;s-Guide"><span>User&#39;s Guide</span></a></li><li><a class="tocitem" href="#Available-Losses"><span>Available Losses</span></a></li><li><a class="tocitem" href="#Advanced-Topics"><span>Advanced Topics</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="introduction/gettingstarted/">Getting Started</a></li><li><a class="tocitem" href="introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="tocitem">User&#39;s Guide</span><ul><li><a class="tocitem" href="user/interface/">Working with Losses</a></li><li><a class="tocitem" href="user/aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="tocitem">Available Losses</span><ul><li><a class="tocitem" href="losses/distance/">Distance-based Losses</a></li><li><a class="tocitem" href="losses/margin/">Margin-based Losses</a></li><li><a class="tocitem" href="losses/other/">Other Losses</a></li></ul></li><li><span class="tocitem">Advances Topics</span><ul><li><a class="tocitem" href="advanced/extend/">Altering existing Losses</a></li><li><a class="tocitem" href="advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="tocitem" href="acknowledgements/">Acknowledgements</a></li><li><a class="tocitem" href="LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaML/LossFunctions.jl/blob/master/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="LossFunctions.jl&#39;s-documentation"><a class="docs-heading-anchor" href="#LossFunctions.jl&#39;s-documentation">LossFunctions.jl&#39;s documentation</a><a id="LossFunctions.jl&#39;s-documentation-1"></a><a class="docs-heading-anchor-permalink" href="#LossFunctions.jl&#39;s-documentation" title="Permalink"></a></h1><p>This package represents a community effort to centralize the definition and implementation of <strong>loss functions</strong> in Julia. As such, it is a part of the <a href="https://github.com/JuliaML">JuliaML</a> ecosystem.</p><p>The sole purpose of this package is to provide an efficient and extensible implementation of various loss functions used throughout Machine Learning (ML). It is thus intended to serve as a special purpose back-end for other ML libraries that require losses to accomplish their tasks. To that end we provide a considerable amount of carefully implemented loss functions, as well as an API to query their properties (e.g. convexity). Furthermore, we expose methods to compute their values, derivatives, and second derivatives for single observations as well as arbitrarily sized arrays of observations. In the case of arrays a user additionally has the ability to define if and how element-wise results are averaged or summed over.</p><p>From an end-user&#39;s perspective one normally does not need to import this package directly. That said, it should provide a decent starting point for any student that is interested in investigating the properties or behaviour of loss functions.</p><h2 id="Introduction-and-Motivation"><a class="docs-heading-anchor" href="#Introduction-and-Motivation">Introduction and Motivation</a><a id="Introduction-and-Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-and-Motivation" title="Permalink"></a></h2><p>If this is the first time you consider using LossFunctions for your machine learning related experiments or packages, make sure to check out the &quot;Getting Started&quot; section.</p><ul><li><a href="introduction/gettingstarted/#Getting-Started">Getting Started</a></li><li class="no-marker"><ul><li><a href="introduction/gettingstarted/#Installation">Installation</a></li><li><a href="introduction/gettingstarted/#Overview">Overview</a></li><li><a href="introduction/gettingstarted/#Getting-Help">Getting Help</a></li></ul></li></ul><p>If you are new to Machine Learning in Julia, or are simply interested in how and why this package works the way it works, feel free to take a look at the following sections. There we discuss the concepts involved and outline the most important terms and definitions.</p><ul><li><a href="introduction/motivation/#Background-and-Motivation">Background and Motivation</a></li><li class="no-marker"><ul><li><a href="introduction/motivation/#Terminology">Terminology</a></li><li><a href="introduction/motivation/#Definitions">Definitions</a></li><li><a href="introduction/motivation/#Alternative-Viewpoints">Alternative Viewpoints</a></li><li><a href="introduction/motivation/#References">References</a></li></ul></li></ul><h2 id="User&#39;s-Guide"><a class="docs-heading-anchor" href="#User&#39;s-Guide">User&#39;s Guide</a><a id="User&#39;s-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#User&#39;s-Guide" title="Permalink"></a></h2><p>This section gives a more detailed treatment of the exposed functions and their available methods. We will start by describing how to instantiate a loss, as well as the basic interface that all loss functions share.</p><ul><li><a href="user/interface/#Working-with-Losses">Working with Losses</a></li><li class="no-marker"><ul><li><a href="user/interface/#Instantiating-a-Loss">Instantiating a Loss</a></li><li><a href="user/interface/#Computing-the-Values">Computing the Values</a></li><li><a href="user/interface/#Computing-the-1st-Derivatives">Computing the 1st Derivatives</a></li><li><a href="user/interface/#Computing-the-2nd-Derivatives">Computing the 2nd Derivatives</a></li><li><a href="user/interface/#Properties-of-a-Loss">Properties of a Loss</a></li></ul></li></ul><p>Next we will consider how to average or sum the results of the loss functions more efficiently. The methods described here are implemented in such a way as to avoid allocating a temporary array.</p><ul><li><a href="user/aggregate/#Efficient-Sum-and-Mean">Efficient Sum and Mean</a></li></ul><h2 id="Available-Losses"><a class="docs-heading-anchor" href="#Available-Losses">Available Losses</a><a id="Available-Losses-1"></a><a class="docs-heading-anchor-permalink" href="#Available-Losses" title="Permalink"></a></h2><p>Aside from the interface, this package also provides a number of popular (and not so popular) loss functions out-of-the-box. Great effort has been put into ensuring a correct, efficient, and type-stable implementation for those. Most of them either belong to the family of distance-based or margin-based losses. These two categories are also indicative for if a loss is intended for regression or classification problems</p><h3 id="Loss-Functions-for-Regression"><a class="docs-heading-anchor" href="#Loss-Functions-for-Regression">Loss Functions for Regression</a><a id="Loss-Functions-for-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-Functions-for-Regression" title="Permalink"></a></h3><p>Loss functions that belong to the category &quot;distance-based&quot; are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.</p><table><tbody><tr><td style="text-align: left;"><ul><li><a href="losses/distance/#Distance-based-Losses">Distance-based Losses</a></li><li class="no-marker"><ul><li><a href="losses/distance/#LPDistLoss">LPDistLoss</a></li><li><a href="losses/distance/#L1DistLoss">L1DistLoss</a></li><li><a href="losses/distance/#L2DistLoss">L2DistLoss</a></li><li><a href="losses/distance/#LogitDistLoss">LogitDistLoss</a></li><li><a href="losses/distance/#HuberLoss">HuberLoss</a></li><li><a href="losses/distance/#L1EpsilonInsLoss">L1EpsilonInsLoss</a></li><li><a href="losses/distance/#L2EpsilonInsLoss">L2EpsilonInsLoss</a></li><li><a href="losses/distance/#PeriodicLoss">PeriodicLoss</a></li><li><a href="losses/distance/#QuantileLoss">QuantileLoss</a></li><li><a href="losses/distance/#LogCoshLoss">LogCoshLoss</a></li></ul></li></ul></td><td><p><img src="https://rawgithub.com/JuliaML/FileStorage/master/LossFunctions/distance.svg" alt="distance-based losses"/></p></td></tr></tbody></table><h3 id="Loss-Functions-for-Classification"><a class="docs-heading-anchor" href="#Loss-Functions-for-Classification">Loss Functions for Classification</a><a id="Loss-Functions-for-Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-Functions-for-Classification" title="Permalink"></a></h3><p>Margin-based loss functions are particularly useful for binary classification. In contrast to the distance-based losses, these do not care about the difference between true target and prediction. Instead they penalize predictions based on how well they agree with the sign of the target.</p><table><tbody><tr><td style="text-align: left;"><ul><li><a href="losses/margin/#Margin-based-Losses">Margin-based Losses</a></li><li class="no-marker"><ul><li><a href="losses/margin/#ZeroOneLoss">ZeroOneLoss</a></li><li><a href="losses/margin/#PerceptronLoss">PerceptronLoss</a></li><li><a href="losses/margin/#L1HingeLoss">L1HingeLoss</a></li><li><a href="losses/margin/#SmoothedL1HingeLoss">SmoothedL1HingeLoss</a></li><li><a href="losses/margin/#ModifiedHuberLoss">ModifiedHuberLoss</a></li><li><a href="losses/margin/#DWDMarginLoss">DWDMarginLoss</a></li><li><a href="losses/margin/#L2MarginLoss">L2MarginLoss</a></li><li><a href="losses/margin/#L2HingeLoss">L2HingeLoss</a></li><li><a href="losses/margin/#LogitMarginLoss">LogitMarginLoss</a></li><li><a href="losses/margin/#ExpLoss">ExpLoss</a></li><li><a href="losses/margin/#SigmoidLoss">SigmoidLoss</a></li></ul></li></ul></td><td><p><img src="https://rawgithub.com/JuliaML/FileStorage/master/LossFunctions/margin.svg" alt="margin-based losses"/></p></td></tr></tbody></table><h2 id="Advanced-Topics"><a class="docs-heading-anchor" href="#Advanced-Topics">Advanced Topics</a><a id="Advanced-Topics-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Topics" title="Permalink"></a></h2><p>In some situations it can be useful to slightly alter an existing loss function. We provide two general ways to accomplish that. The first way is to scale a loss by a constant factor. This can for example be useful to transform the <a href="losses/distance/#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a> into the least squares loss one knows from statistics. The second way is to reweight the two classes of a binary classification loss. This is useful for handling inbalanced class distributions.</p><ul><li><a href="advanced/extend/#Altering-existing-Losses">Altering existing Losses</a></li><li class="no-marker"><ul><li><a href="advanced/extend/#Scaling-a-Supervised-Loss">Scaling a Supervised Loss</a></li><li><a href="advanced/extend/#Reweighting-a-Margin-Loss">Reweighting a Margin Loss</a></li></ul></li></ul><p>If you are interested in contributing to LossFunctions.jl, or simply want to understand how and why the package does then take a look at our developer documentation (although it is a bit sparse at the moment).</p><ul><li><a href="advanced/developer/#Developer-Documentation">Developer Documentation</a></li><li class="no-marker"><ul><li><a href="advanced/developer/#Abstract-Types">Abstract Types</a></li><li><a href="advanced/developer/#Shared-Interface">Shared Interface</a></li></ul></li></ul><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li class="no-marker"><ul><li><a href="indices/#Functions">Functions</a></li><li><a href="indices/#Types">Types</a></li></ul></li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="introduction/gettingstarted/">Getting Started »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 27 April 2023 19:36">Thursday 27 April 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
