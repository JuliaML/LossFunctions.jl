<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Working with Losses · LossFunctions.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaML.github.io/LossFunctions.jl/user/interface/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LossFunctions.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="tocitem" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="tocitem">User&#39;s Guide</span><ul><li class="is-active"><a class="tocitem" href>Working with Losses</a><ul class="internal"><li><a class="tocitem" href="#Instantiating-a-Loss"><span>Instantiating a Loss</span></a></li><li><a class="tocitem" href="#Computing-the-Values"><span>Computing the Values</span></a></li><li><a class="tocitem" href="#Computing-the-1st-Derivatives"><span>Computing the 1st Derivatives</span></a></li><li><a class="tocitem" href="#Computing-the-2nd-Derivatives"><span>Computing the 2nd Derivatives</span></a></li><li><a class="tocitem" href="#Properties-of-a-Loss"><span>Properties of a Loss</span></a></li></ul></li><li><a class="tocitem" href="../aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="tocitem">Available Losses</span><ul><li><a class="tocitem" href="../../losses/distance/">Distance-based Losses</a></li><li><a class="tocitem" href="../../losses/margin/">Margin-based Losses</a></li><li><a class="tocitem" href="../../losses/other/">Other Losses</a></li></ul></li><li><span class="tocitem">Advances Topics</span><ul><li><a class="tocitem" href="../../advanced/extend/">Altering existing Losses</a></li><li><a class="tocitem" href="../../advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="tocitem" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="tocitem" href="../../LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User&#39;s Guide</a></li><li class="is-active"><a href>Working with Losses</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Working with Losses</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaML/LossFunctions.jl/blob/master/docs/src/user/interface.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Working-with-Losses"><a class="docs-heading-anchor" href="#Working-with-Losses">Working with Losses</a><a id="Working-with-Losses-1"></a><a class="docs-heading-anchor-permalink" href="#Working-with-Losses" title="Permalink"></a></h1><p>Even though they are called loss &quot;functions&quot;, this package implements them as immutable types instead of true Julia functions. There are good reasons for that. For example it allows us to specify the properties of loss functions explicitly (e.g. <code>isconvex(myloss)</code>). It also makes for a more consistent API when it comes to computing the value or the derivative. Some loss functions even have additional parameters that need to be specified, such as the <span>$\epsilon$</span> in the case of the <span>$\epsilon$</span>-insensitive loss. Here, types allow for member variables to hide that information away from the method signatures.</p><p>In order to avoid potential confusions with true Julia functions, we will refer to &quot;loss functions&quot; as &quot;losses&quot; instead. The available losses share a common interface for the most part. This section will provide an overview of the basic functionality that is available for all the different types of losses. We will discuss how to create a loss, how to compute its value and derivative, and how to query its properties.</p><h2 id="Instantiating-a-Loss"><a class="docs-heading-anchor" href="#Instantiating-a-Loss">Instantiating a Loss</a><a id="Instantiating-a-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Instantiating-a-Loss" title="Permalink"></a></h2><p>Losses are immutable types. As such, one has to instantiate one in order to work with it. For most losses, the constructors do not expect any parameters.</p><pre><code class="language-julia-repl hljs">julia&gt; L2DistLoss()
L2DistLoss()

julia&gt; HingeLoss()
L1HingeLoss()</code></pre><p>We just said that we need to instantiate a loss in order to work with it. One could be inclined to belief, that it would be more memory-efficient to &quot;pre-allocate&quot; a loss when using it in more than one place.</p><pre><code class="language-julia-repl hljs">julia&gt; loss = L2DistLoss()
L2DistLoss()

julia&gt; value(loss, 3, 2)
1</code></pre><p>However, that is a common oversimplification. Because all losses are immutable types, they can live on the stack and thus do not come with a heap-allocation overhead.</p><p>Even more interesting in the example above, is that for such losses as <a href="../../losses/distance/#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a>, which do not have any constructor parameters or member variables, there is no additional code executed at all. Such singletons are only used for dispatch and don&#39;t even produce any additional code, which you can observe for yourself in the code below. As such they are zero-cost abstractions.</p><pre><code class="language-julia-repl hljs">julia&gt; v1(loss,y,t) = value(loss,y,t)

julia&gt; v2(y,t) = value(L2DistLoss(),y,t)

julia&gt; @code_llvm v1(loss, 3, 2)
define i64 @julia_v1_70944(i64, i64) #0 {
top:
  %2 = sub i64 %1, %0
  %3 = mul i64 %2, %2
  ret i64 %3
}

julia&gt; @code_llvm v2(3, 2)
define i64 @julia_v2_70949(i64, i64) #0 {
top:
  %2 = sub i64 %1, %0
  %3 = mul i64 %2, %2
  ret i64 %3
}</code></pre><p>On the other hand, some types of losses are actually more comparable to whole families of losses instead of just a single one. For example, the immutable type <a href="../../losses/distance/#LossFunctions.L1EpsilonInsLoss"><code>L1EpsilonInsLoss</code></a> has a free parameter <span>$\epsilon$</span>. Each concrete <span>$\epsilon$</span> results in a different concrete loss of the same family of epsilon-insensitive losses.</p><pre><code class="language-julia-repl hljs">julia&gt; L1EpsilonInsLoss(0.5)
L1EpsilonInsLoss{Float64}(0.5)

julia&gt; L1EpsilonInsLoss(1)
L1EpsilonInsLoss{Float64}(1.0)</code></pre><p>For such losses that do have parameters, it can make a slight difference to pre-instantiate a loss. While they will live on the stack, the constructor usually performs some assertions and conversion for the given parameter. This can come at a slight overhead. At the very least it will not produce the same exact code when pre-instantiated. Still, the fact that they are immutable makes them very efficient abstractions with little to no performance overhead, and zero memory allocations on the heap.</p><h2 id="Computing-the-Values"><a class="docs-heading-anchor" href="#Computing-the-Values">Computing the Values</a><a id="Computing-the-Values-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-Values" title="Permalink"></a></h2><p>The first thing we may want to do is compute the loss for some observation (singular). In fact, all losses are implemented on single observations under the hood. The core function to compute the value of a loss is <code>value</code>. We will see throughout the documentation that this function allows for a lot of different method signatures to accomplish a variety of tasks.</p><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.value" href="#LossFunctions.value"><code>LossFunctions.value</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">value(loss, output, target) -&gt; Number</code></pre><p>Compute the (non-negative) numeric result for the <code>loss</code> function. Note that <code>target</code> and <code>output</code> can be of different numeric type, in which case promotion is performed in the manner appropriate for the given loss.</p><pre><code class="nohighlight hljs">value(loss, outputs, targets) -&gt; AbstractVector</code></pre><p>Compute the result for each pair of values in <code>targets</code> and <code>outputs</code>.</p><pre><code class="nohighlight hljs">value(loss, outputs, targets, aggmode) -&gt; Number</code></pre><p>Compute the weighted or unweighted sum or mean (depending on aggregation mode <code>aggmode</code>) of the individual values of the <code>loss</code> function for each pair in <code>targets</code> and <code>outputs</code>. This method will not allocate a temporary array.</p><p><strong>Notes</strong></p><ul><li>New loss functions only need to implement the first method with single <code>target</code> and <code>output</code>. Fallback implementations are available for other methods in <code>LossFunctions.jl</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L35-L59">source</a></section></article><p>It may be interesting to note, that this function also supports broadcasting and all the syntax benefits that come with it. Thus, it is quite simple to make use of preallocated memory for storing the element-wise results.</p><pre><code class="language-julia-repl hljs">julia&gt; value.(L1DistLoss(), [2,5,-2], [1,2,3])
3-element Vector{Int64}:
 1
 3
 5

julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; buffer .= value.(L1DistLoss(), [2,5,-2], [1.,2,3])
3-element Vector{Float64}:
 1.0
 3.0
 5.0</code></pre><p>Furthermore, with the loop fusion changes that were introduced in Julia 0.6, one can also easily weight the influence of each observation without allocating a temporary array.</p><pre><code class="language-julia-repl hljs">julia&gt; buffer .= value.(L1DistLoss(), [2,5,-2], [1.,2,3]) .* [2,1,0.5]
3-element Vector{Float64}:
 2.0
 3.0
 2.5</code></pre><h2 id="Computing-the-1st-Derivatives"><a class="docs-heading-anchor" href="#Computing-the-1st-Derivatives">Computing the 1st Derivatives</a><a id="Computing-the-1st-Derivatives-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-1st-Derivatives" title="Permalink"></a></h2><p>Maybe the more interesting aspect of loss functions are their derivatives. In fact, most of the popular learning algorithm in Supervised Learning, such as gradient descent, utilize the derivatives of the loss in one way or the other during the training process.</p><p>To compute the derivative of some loss we expose the function <a href="#LossFunctions.deriv"><code>deriv</code></a>. It supports the same exact method signatures as <a href="#LossFunctions.value"><code>value</code></a>. It may be interesting to note explicitly, that we always compute the derivative in respect to the predicted <code>output</code>, since we are interested in deducing in which direction the output should change.</p><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.deriv" href="#LossFunctions.deriv"><code>LossFunctions.deriv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">deriv(loss, output, target) -&gt; Number</code></pre><p>Compute the analytical derivative with respect to the <code>output</code> for the <code>loss</code> function. Note that <code>target</code> and <code>output</code> can be of different numeric type, in which case promotion is performed in the manner appropriate for the given loss.</p><pre><code class="nohighlight hljs">deriv(loss, outputs, targets) -&gt; AbstractVector</code></pre><p>Compute the result for each pair of values in <code>targets</code> and <code>outputs</code>.</p><pre><code class="nohighlight hljs">deriv(loss, outputs, targets, aggmode) -&gt; Number</code></pre><p>Compute the weighted or unweighted sum or mean (depending on aggregation mode <code>aggmode</code>) of the individual values of the <code>loss</code> function for each pair in <code>targets</code> and <code>outputs</code>. This method will not allocate a temporary array.</p><p><strong>Notes</strong></p><ul><li>New loss functions only need to implement the first method with single <code>target</code> and <code>output</code>. Fallback implementations are available for other methods in <code>LossFunctions.jl</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L62-L86">source</a></section></article><p>Similar to <a href="#LossFunctions.value"><code>value</code></a>, this function also supports broadcasting and all the syntax benefits that come with it. Thus, one can make use of preallocated memory for storing the element-wise derivatives.</p><pre><code class="language-julia-repl hljs">julia&gt; deriv.(L2DistLoss(), [2,5,-2], [1,2,3])
3-element Vector{Int64}:
   2
   6
 -10

julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; buffer .= deriv.(L2DistLoss(), [2,5,-2], [1.,2,3])
3-element Vector{Float64}:
   2.0
   6.0
 -10.0</code></pre><p>Furthermore, with the loop fusion changes that were introduced in Julia 0.6, one can also easily weight the influence of each observation without allocating a temporary array.</p><pre><code class="language-julia-repl hljs">julia&gt; buffer .= deriv.(L2DistLoss(), [2,5,-2], [1.,2,3]) .* [2,1,0.5]
3-element Vector{Float64}:
  4.0
  6.0
 -5.0</code></pre><h2 id="Computing-the-2nd-Derivatives"><a class="docs-heading-anchor" href="#Computing-the-2nd-Derivatives">Computing the 2nd Derivatives</a><a id="Computing-the-2nd-Derivatives-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-2nd-Derivatives" title="Permalink"></a></h2><p>Additionally to the first derivative, we also provide the corresponding methods for the second derivative through the function <a href="#LossFunctions.deriv2"><code>deriv2</code></a>. Note again, that we always compute the derivative in respect to the predicted <code>output</code>.</p><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.deriv2" href="#LossFunctions.deriv2"><code>LossFunctions.deriv2</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">deriv2(loss, output, target) -&gt; Number</code></pre><p>Compute the second derivative with respect to the <code>output</code> for the <code>loss</code> function. Note that <code>target</code> and <code>output</code> can be of different numeric type, in which case promotion is performed in the manner appropriate for the given loss.</p><pre><code class="nohighlight hljs">deriv2(loss, outputs, targets) -&gt; AbstractVector</code></pre><p>Compute the result for each pair of values in <code>targets</code> and <code>outputs</code>.</p><pre><code class="nohighlight hljs">deriv2(loss, outputs, targets, aggmode) -&gt; Number</code></pre><p>Compute the weighted or unweighted sum or mean (depending on aggregation mode <code>aggmode</code>) of the individual values of the <code>loss</code> function for each pair in <code>targets</code> and <code>outputs</code>. This method will not allocate a temporary array.</p><p><strong>Notes</strong></p><ul><li>New loss functions only need to implement the first method with single <code>target</code> and <code>output</code>. Fallback implementations are available for other methods in <code>LossFunctions.jl</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L89-L113">source</a></section></article><p>Just like <a href="#LossFunctions.deriv"><code>deriv</code></a> and <a href="#LossFunctions.value"><code>value</code></a>, this function also supports broadcasting and all the syntax benefits that come with it. Thus, one can make use of preallocated memory for storing the element-wise derivatives.</p><pre><code class="language-julia-repl hljs">julia&gt; deriv2.(LogitDistLoss(), [0.3, 2.3, -2], [-0.5, 1.2, 3])
3-element Vector{Float64}:
 0.42781939304058886
 0.3747397590950413
 0.013296113341580313

julia&gt; buffer = zeros(3); # preallocate a buffer

julia&gt; buffer .= deriv2.(LogitDistLoss(), [0.3, 2.3, -2], [-0.5, 1.2, 3])
3-element Vector{Float64}:
 0.42781939304058886
 0.3747397590950413
 0.013296113341580313</code></pre><p>Furthermore <a href="#LossFunctions.deriv2"><code>deriv2</code></a> supports all the same method signatures as <a href="#LossFunctions.deriv"><code>deriv</code></a> does.</p><h2 id="Properties-of-a-Loss"><a class="docs-heading-anchor" href="#Properties-of-a-Loss">Properties of a Loss</a><a id="Properties-of-a-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Properties-of-a-Loss" title="Permalink"></a></h2><p>In some situations it can be quite useful to assert certain properties about a loss-function. One such scenario could be when implementing an algorithm that requires the loss to be strictly convex or Lipschitz continuous. Note that we will only skim over the defintions in most cases. A good treatment of all of the concepts involved can be found in either <sup class="footnote-reference"><a id="citeref-BOYD2004" href="#footnote-BOYD2004">[BOYD2004]</a></sup> or <sup class="footnote-reference"><a id="citeref-STEINWART2008" href="#footnote-STEINWART2008">[STEINWART2008]</a></sup>.</p><p>This package uses functions to represent individual properties of a loss. It follows a list of implemented property-functions defined in <a href="https://github.com/JuliaML/LearnBase.jl">LearnBase.jl</a>.</p><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isconvex" href="#LossFunctions.isconvex"><code>LossFunctions.isconvex</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isconvex(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a convex function. A function <code>f: ℝⁿ → ℝ</code> is convex if its domain is a convex set and if for all <code>x, y</code> in that domain, with <code>θ</code> such that for <code>0 ≦ θ ≦ 1</code>, we have <code>f(θ x + (1 - θ) y) ≦ θ f(x) + (1 - θ) f(y)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L116-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isstrictlyconvex" href="#LossFunctions.isstrictlyconvex"><code>LossFunctions.isstrictlyconvex</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isstrictlyconvex(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a strictly convex function. A function <code>f : ℝⁿ → ℝ</code> is strictly convex if its domain is a convex set and if for all <code>x, y</code> in that domain where <code>x ≠ y</code>, with <code>θ</code> such that for <code>0 &lt; θ &lt; 1</code>, we have <code>f(θ x + (1 - θ) y) &lt; θ f(x) + (1 - θ) f(y)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L126-L133">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isstronglyconvex" href="#LossFunctions.isstronglyconvex"><code>LossFunctions.isstronglyconvex</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isstronglyconvex(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a strongly convex function. A function <code>f : ℝⁿ → ℝ</code> is <code>m</code>-strongly convex if its domain is a convex set, and if for all <code>x, y</code> in that domain where <code>x ≠ y</code>, and <code>θ</code> such that for <code>0 ≤ θ ≤ 1</code>, we have <code>f(θ x + (1 - θ)y) &lt; θ f(x) + (1 - θ) f(y) - 0.5 m ⋅ θ (1 - θ) | x - y |₂²</code></p><p>In a more familiar setting, if the loss function is differentiable we have <code>(∇f(x) - ∇f(y))ᵀ (x - y) ≥ m | x - y |₂²</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L136-L147">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isdifferentiable" href="#LossFunctions.isdifferentiable"><code>LossFunctions.isdifferentiable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isdifferentiable(loss, [x]) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> is differentiable (optionally limited to the given point <code>x</code> if specified).</p><p>A function <code>f : ℝⁿ → ℝᵐ</code> is differentiable at a point <code>x</code> in the interior domain of <code>f</code> if there exists a matrix <code>Df(x) ∈ ℝ^(m × n)</code> such that it satisfies:</p><p><code>lim_{z ≠ x, z → x} (|f(z) - f(x) - Df(x)(z-x)|₂) / |z - x|₂ = 0</code></p><p>A function is differentiable if its domain is open and it is differentiable at every point <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L150-L164">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.istwicedifferentiable" href="#LossFunctions.istwicedifferentiable"><code>LossFunctions.istwicedifferentiable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">istwicedifferentiable(loss, [x]) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> is differentiable (optionally limited to the given point <code>x</code> if specified).</p><p>A function <code>f : ℝⁿ → ℝ</code> is said to be twice differentiable at a point <code>x</code> in the interior domain of <code>f</code>, if the function derivative for <code>∇f</code> exists at <code>x</code>: <code>∇²f(x) = D∇f(x)</code>.</p><p>A function is twice differentiable if its domain is open and it is twice differentiable at every point <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L168-L180">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.islocallylipschitzcont" href="#LossFunctions.islocallylipschitzcont"><code>LossFunctions.islocallylipschitzcont</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">islocallylipschitzcont(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> function is locally-Lipschitz continous.</p><p>A supervised loss <code>L : Y × ℝ → [0, ∞)</code> is called locally Lipschitz continuous if for all <code>a ≥ 0</code> there exists a constant <code>cₐ ≥ 0</code>, such that</p><p><code>sup_{y ∈ Y} | L(y,t) − L(y,t′) | ≤ cₐ |t − t′|, t, t′ ∈ [−a,a]</code></p><p>Every convex function is locally lipschitz continuous.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L184-L197">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.islipschitzcont" href="#LossFunctions.islipschitzcont"><code>LossFunctions.islipschitzcont</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">islipschitzcont(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> function is Lipschitz continuous.</p><p>A supervised loss function <code>L : Y × ℝ → [0, ∞)</code> is Lipschitz continous, if there exists a finite constant <code>M &lt; ∞</code> such that <code>|L(y, t) - L(y, t′)| ≤ M |t - t′|, ∀ (y, t) ∈ Y × ℝ</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L201-L209">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isnemitski" href="#LossFunctions.isnemitski"><code>LossFunctions.isnemitski</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isnemitski(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> denotes a Nemitski loss function.</p><p>We call a supervised loss function <code>L : Y × ℝ → [0,∞)</code> a Nemitski loss if there exist a measurable function <code>b : Y → [0, ∞)</code> and an increasing function <code>h : [0, ∞) → [0, ∞)</code> such that <code>L(y,ŷ) ≤ b(y) + h(|ŷ|), (y, ŷ) ∈ Y × ℝ</code></p><p>If a loss if locally lipsschitz continuous then it is a Nemitski loss.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L212-L223">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isclipable" href="#LossFunctions.isclipable"><code>LossFunctions.isclipable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isclipable(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> function is clipable. A supervised loss <code>L : Y × ℝ → [0,∞)</code> can be clipped at <code>M &gt; 0</code> if, for all <code>(y,t) ∈ Y × ℝ</code>, <code>L(y, t̂) ≤ L(y, t)</code> where <code>t̂</code> denotes the clipped value of <code>t</code> at <code>± M</code>. That is <code>t̂ = -M</code> if <code>t &lt; -M</code>, <code>t̂ = t</code> if <code>t ∈ [-M, M]</code>, and <code>t = M</code> if <code>t &gt; M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L243-L252">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.ismarginbased" href="#LossFunctions.ismarginbased"><code>LossFunctions.ismarginbased</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ismarginbased(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> is a margin-based loss.</p><p>A supervised loss function <code>L : Y × ℝ → [0,∞)</code> is said to be margin-based, if there exists a representing function <code>ψ : ℝ → [0,∞)</code> satisfying <code>L(y, ŷ) = ψ(y⋅ŷ), (y, ŷ) ∈ Y × ℝ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L268-L276">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isclasscalibrated" href="#LossFunctions.isclasscalibrated"><code>LossFunctions.isclasscalibrated</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isclasscalibrated(loss) -&gt; Bool</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L280-L282">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.isdistancebased" href="#LossFunctions.isdistancebased"><code>LossFunctions.isdistancebased</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">isdistancebased(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given <code>loss</code> is a distance-based loss.</p><p>A supervised loss function <code>L : Y × ℝ → [0,∞)</code> is said to be distance-based, if there exists a representing function <code>ψ : ℝ → [0,∞)</code> satisfying <code>ψ(0) = 0</code> and <code>L(y, ŷ) = ψ (ŷ - y), (y, ŷ) ∈ Y × ℝ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L256-L264">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.issymmetric" href="#LossFunctions.issymmetric"><code>LossFunctions.issymmetric</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">issymmetric(loss) -&gt; Bool</code></pre><p>Return <code>true</code> if the given loss is a symmetric loss.</p><p>A function <code>f : ℝ → [0,∞)</code> is said to be symmetric about origin if we have <code>f(x) = f(-x), ∀ x ∈ ℝ</code>.</p><p>A distance-based loss is said to be symmetric if its representing function is symmetric.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/143b35fcaf75d7f3895c97673235992051a42801/src/traits.jl#L287-L297">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-BOYD2004"><a class="tag is-link" href="#citeref-BOYD2004">BOYD2004</a>Stephen Boyd and Lieven Vandenberghe. <a href="https://stanford.edu/~boyd/cvxbook/">&quot;Convex Optimization&quot;</a>. Cambridge University Press, 2004.</li><li class="footnote" id="footnote-STEINWART2008"><a class="tag is-link" href="#citeref-STEINWART2008">STEINWART2008</a>Steinwart, Ingo, and Andreas Christmann. <a href="https://www.springer.com/us/book/9780387772417">&quot;Support vector machines&quot;</a>. Springer Science &amp; Business Media, 2008.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../introduction/motivation/">« Background and Motivation</a><a class="docs-footer-nextpage" href="../aggregate/">Efficient Sum and Mean »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 24 April 2023 21:29">Monday 24 April 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
