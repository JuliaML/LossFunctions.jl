<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distance-based Losses · LossFunctions.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaML.github.io/LossFunctions.jl/losses/distance/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LossFunctions.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="tocitem" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="tocitem">User&#39;s Guide</span><ul><li><a class="tocitem" href="../../user/interface/">Working with Losses</a></li><li><a class="tocitem" href="../../user/aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="tocitem">Available Losses</span><ul><li class="is-active"><a class="tocitem" href>Distance-based Losses</a><ul class="internal"><li><a class="tocitem" href="#LPDistLoss"><span>LPDistLoss</span></a></li><li><a class="tocitem" href="#L1DistLoss"><span>L1DistLoss</span></a></li><li><a class="tocitem" href="#L2DistLoss"><span>L2DistLoss</span></a></li><li><a class="tocitem" href="#LogitDistLoss"><span>LogitDistLoss</span></a></li><li><a class="tocitem" href="#HuberLoss"><span>HuberLoss</span></a></li><li><a class="tocitem" href="#L1EpsilonInsLoss"><span>L1EpsilonInsLoss</span></a></li><li><a class="tocitem" href="#L2EpsilonInsLoss"><span>L2EpsilonInsLoss</span></a></li><li><a class="tocitem" href="#PeriodicLoss"><span>PeriodicLoss</span></a></li><li><a class="tocitem" href="#QuantileLoss"><span>QuantileLoss</span></a></li><li><a class="tocitem" href="#LogCoshLoss"><span>LogCoshLoss</span></a></li></ul></li><li><a class="tocitem" href="../margin/">Margin-based Losses</a></li><li><a class="tocitem" href="../other/">Other Losses</a></li></ul></li><li><span class="tocitem">Advances Topics</span><ul><li><a class="tocitem" href="../../advanced/extend/">Altering existing Losses</a></li><li><a class="tocitem" href="../../advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="tocitem" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="tocitem" href="../../LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Available Losses</a></li><li class="is-active"><a href>Distance-based Losses</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distance-based Losses</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaML/LossFunctions.jl/blob/master/docs/src/losses/distance.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Distance-based-Losses"><a class="docs-heading-anchor" href="#Distance-based-Losses">Distance-based Losses</a><a id="Distance-based-Losses-1"></a><a class="docs-heading-anchor-permalink" href="#Distance-based-Losses" title="Permalink"></a></h1><p>Loss functions that belong to the category &quot;distance-based&quot; are primarily used in regression problems. They utilize the numeric difference between the predicted output and the true target as a proxy variable to quantify the quality of individual predictions.</p><p>This section lists all the subtypes of <a href="losses/@ref"><code>DistanceLoss</code></a> that are implemented in this package.</p><h2 id="LPDistLoss"><a class="docs-heading-anchor" href="#LPDistLoss">LPDistLoss</a><a id="LPDistLoss-1"></a><a class="docs-heading-anchor-permalink" href="#LPDistLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.LPDistLoss" href="#LossFunctions.LPDistLoss"><code>LossFunctions.LPDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LPDistLoss{P} &lt;: DistanceLoss</code></pre><p>The P-th power absolute distance loss. It is Lipschitz continuous iff <code>P == 1</code>, convex if and only if <code>P &gt;= 1</code>, and strictly convex iff <code>P &gt; 1</code>.</p><p class="math-container">\[L(r) = |r|^P\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L5">source</a></section></article><h2 id="L1DistLoss"><a class="docs-heading-anchor" href="#L1DistLoss">L1DistLoss</a><a id="L1DistLoss-1"></a><a class="docs-heading-anchor-permalink" href="#L1DistLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L1DistLoss" href="#LossFunctions.L1DistLoss"><code>LossFunctions.L1DistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L1DistLoss &lt;: DistanceLoss</code></pre><p>The absolute distance loss. Special case of the <a href="#LossFunctions.LPDistLoss"><code>LPDistLoss</code></a> with <code>P=1</code>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(r) = |r|\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    3 │\.                     ./│    1 │            ┌------------│
      │ &#39;\.                 ./&#39; │      │            |            │
      │   \.               ./   │      │            |            │
      │    &#39;\.           ./&#39;    │      │_           |           _│
    L │      \.         ./      │   L&#39; │            |            │
      │       &#39;\.     ./&#39;       │      │            |            │
      │         \.   ./         │      │            |            │
    0 │          &#39;\./&#39;          │   -1 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -3                        3
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L50">source</a></section></article><h2 id="L2DistLoss"><a class="docs-heading-anchor" href="#L2DistLoss">L2DistLoss</a><a id="L2DistLoss-1"></a><a class="docs-heading-anchor-permalink" href="#L2DistLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L2DistLoss" href="#LossFunctions.L2DistLoss"><code>LossFunctions.L2DistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2DistLoss &lt;: DistanceLoss</code></pre><p>The least squares loss. Special case of the <a href="#LossFunctions.LPDistLoss"><code>LPDistLoss</code></a> with <code>P=2</code>. It is strictly convex.</p><p class="math-container">\[L(r) = |r|^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    9 │\                       /│    3 │                   .r/   │
      │&quot;.                     .&quot;│      │                 .r&#39;     │
      │ &quot;.                   .&quot; │      │              _./&#39;       │
      │  &quot;.                 .&quot;  │      │_           .r/         _│
    L │   &quot;.               .&quot;   │   L&#39; │         _:/&#39;            │
      │    &#39;\.           ./&#39;    │      │       .r&#39;               │
      │      \.         ./      │      │     .r&#39;                 │
    0 │        &quot;-.___.-&quot;        │   -3 │  _/r&#39;                   │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L95">source</a></section></article><h2 id="LogitDistLoss"><a class="docs-heading-anchor" href="#LogitDistLoss">LogitDistLoss</a><a id="LogitDistLoss-1"></a><a class="docs-heading-anchor-permalink" href="#LogitDistLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.LogitDistLoss" href="#LossFunctions.LogitDistLoss"><code>LossFunctions.LogitDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogitDistLoss &lt;: DistanceLoss</code></pre><p>The distance-based logistic loss for regression. It is strictly convex and Lipschitz continuous.</p><p class="math-container">\[L(r) = - \ln \frac{4 e^r}{(1 + e^r)^2}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │                         │    1 │                   _--&#39;&#39;&#39;│
      │\                       /│      │                ./&#39;      │
      │ \.                   ./ │      │              ./         │
      │  &#39;.                 .&#39;  │      │_           ./          _│
    L │   &#39;.               .&#39;   │   L&#39; │           ./            │
      │     \.           ./     │      │         ./              │
      │      &#39;.         .&#39;      │      │       ./                │
    0 │        &#39;-.___.-&#39;        │   -1 │___.-&#39;&#39;                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -4                        4
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L376">source</a></section></article><h2 id="HuberLoss"><a class="docs-heading-anchor" href="#HuberLoss">HuberLoss</a><a id="HuberLoss-1"></a><a class="docs-heading-anchor-permalink" href="#HuberLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.HuberLoss" href="#LossFunctions.HuberLoss"><code>LossFunctions.HuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HuberLoss &lt;: DistanceLoss</code></pre><p>Loss function commonly used for robustness to outliers. For large values of <code>d</code> it becomes close to the <a href="#LossFunctions.L1DistLoss"><code>L1DistLoss</code></a>, while for small values of <code>d</code> it resembles the <a href="#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a>. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(r) = \begin{cases} \frac{r^2}{2} &amp; \quad \text{if } | r | \le \alpha \\ \alpha | r | - \frac{\alpha^3}{2} &amp; \quad \text{otherwise}\\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (d=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │                         │    1 │                .+-------│
      │                         │      │              ./&#39;        │
      │\.                     ./│      │             ./          │
      │ &#39;.                   .&#39; │      │_           ./          _│
    L │   \.               ./   │   L&#39; │           /&#39;            │
      │     \.           ./     │      │          /&#39;             │
      │      &#39;.         .&#39;      │      │        ./&#39;              │
    0 │        &#39;-.___.-&#39;        │   -1 │-------+&#39;                │
      └────────────┴────────────┘      └────────────┴────────────┘
      -2                        2      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L174">source</a></section></article><h2 id="L1EpsilonInsLoss"><a class="docs-heading-anchor" href="#L1EpsilonInsLoss">L1EpsilonInsLoss</a><a id="L1EpsilonInsLoss-1"></a><a class="docs-heading-anchor-permalink" href="#L1EpsilonInsLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L1EpsilonInsLoss" href="#LossFunctions.L1EpsilonInsLoss"><code>LossFunctions.L1EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L1EpsilonInsLoss &lt;: DistanceLoss</code></pre><p>The <span>$ϵ$</span>-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than <span>$ϵ$</span>, but penalizes larger deviances linarily. It is Lipschitz continuous and convex, but not strictly convex.</p><p class="math-container">\[L(r) = \max \{ 0, | r | - \epsilon \}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (ϵ=1)               Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │\                       /│    1 │                  ┌------│
      │ \                     / │      │                  |      │
      │  \                   /  │      │                  |      │
      │   \                 /   │      │_      ___________!     _│
    L │    \               /    │   L&#39; │      |                  │
      │     \             /     │      │      |                  │
      │      \           /      │      │      |                  │
    0 │       \_________/       │   -1 │------┘                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L247">source</a></section></article><h2 id="L2EpsilonInsLoss"><a class="docs-heading-anchor" href="#L2EpsilonInsLoss">L2EpsilonInsLoss</a><a id="L2EpsilonInsLoss-1"></a><a class="docs-heading-anchor-permalink" href="#L2EpsilonInsLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L2EpsilonInsLoss" href="#LossFunctions.L2EpsilonInsLoss"><code>LossFunctions.L2EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">L2EpsilonInsLoss &lt;: DistanceLoss</code></pre><p>The quadratic <span>$ϵ$</span>-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than <span>$ϵ$</span>, but penalizes larger deviances quadratically. It is convex, but not strictly convex.</p><p class="math-container">\[L(r) = \max \{ 0, | r | - \epsilon \}^2\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (ϵ=0.5)             Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    8 │                         │    1 │                  /      │
      │:                       :│      │                 /       │
      │&#39;.                     .&#39;│      │                /        │
      │ \.                   ./ │      │_         _____/        _│
    L │  \.                 ./  │   L&#39; │         /               │
      │   \.               ./   │      │        /                │
      │    &#39;\.           ./&#39;    │      │       /                 │
    0 │      &#39;-._______.-&#39;      │   -1 │      /                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -2                        2
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L310">source</a></section></article><h2 id="PeriodicLoss"><a class="docs-heading-anchor" href="#PeriodicLoss">PeriodicLoss</a><a id="PeriodicLoss-1"></a><a class="docs-heading-anchor-permalink" href="#PeriodicLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.PeriodicLoss" href="#LossFunctions.PeriodicLoss"><code>LossFunctions.PeriodicLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PeriodicLoss &lt;: DistanceLoss</code></pre><p>Measures distance on a circle of specified circumference <code>c</code>.</p><p class="math-container">\[L(r) = 1 - \cos \left( \frac{2 r \pi}{c} \right)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L140">source</a></section></article><h2 id="QuantileLoss"><a class="docs-heading-anchor" href="#QuantileLoss">QuantileLoss</a><a id="QuantileLoss-1"></a><a class="docs-heading-anchor-permalink" href="#QuantileLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.QuantileLoss" href="#LossFunctions.QuantileLoss"><code>LossFunctions.QuantileLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuantileLoss &lt;: DistanceLoss</code></pre><p>The distance-based quantile loss, also known as pinball loss, can be used to estimate conditional τ-quantiles. It is Lipschitz continuous and convex, but not strictly convex. Furthermore it is symmetric if and only if <code>τ = 1/2</code>.</p><p class="math-container">\[L(r) = \begin{cases} -\left( 1 - \tau  \right) r &amp; \quad \text{if } r &lt; 0 \\ \tau r &amp; \quad \text{if } r \ge 0 \\ \end{cases}\]</p><hr/><pre><code class="nohighlight hljs">              Lossfunction (τ=0.7)             Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
    2 │&#39;\                       │  0.3 │            ┌------------│
      │  \.                     │      │            |            │
      │   &#39;\                    │      │_           |           _│
      │     \.                  │      │            |            │
    L │      &#39;\              ._-│   L&#39; │            |            │
      │        \.         ..-&#39;  │      │            |            │
      │         &#39;.     _r/&#39;     │      │            |            │
    0 │           &#39;_./&#39;         │ -0.7 │------------┘            │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -3                        3
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L430">source</a></section></article><h2 id="LogCoshLoss"><a class="docs-heading-anchor" href="#LogCoshLoss">LogCoshLoss</a><a id="LogCoshLoss-1"></a><a class="docs-heading-anchor-permalink" href="#LogCoshLoss" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.LogCoshLoss" href="#LossFunctions.LogCoshLoss"><code>LossFunctions.LogCoshLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogCoshLoss &lt;: DistanceLoss</code></pre><p>The log cosh loss is twice differentiable, strongly convex, Lipschitz continous function.</p><p class="math-container">\[L(r) = log ( cosh ( x ))\]</p><hr/><pre><code class="nohighlight hljs">           Lossfunction                     Derivative
      ┌────────────┬────────────┐      ┌────────────┬────────────┐
  2.5 │\                       /│    1 │                 .-------│
      │&quot;.                     .&quot;│      │                |        │
      │ &quot;.                   .&quot; │      │               /         │
      │  &quot;.                 .&quot;  │      │_           . &quot;         _│
    L │   &quot;.               .&quot;   │   L&#39; │         /&quot;              │
      │    &#39;\.           ./&#39;    │      │       .&quot;                │
      │      \.         ./      │      │       |                 │
    0 │        &quot;-. _ .-&quot;        │   -1 │------&quot;                  │
      └────────────┴────────────┘      └────────────┴────────────┘
      -3                        3      -3                        3
                 ŷ - y                            ŷ - y</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaML/LossFunctions.jl/blob/8be7671571b536b6f3705b6e1bbc7af29b0beb2c/src/losses/distance.jl#L486">source</a></section></article><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>You may note that our definition of the QuantileLoss looks different to what one usually sees in other literature. The reason is that we have to correct for the fact that in our case <span>$r = \hat{y} - y$</span> instead of <span>$r_{\textrm{usual}} = y - \hat{y}$</span>, which means that our definition relates to that in the manner of <span>$r = -1 * r_{\textrm{usual}}$</span>.</p></div></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../user/aggregate/">« Efficient Sum and Mean</a><a class="docs-footer-nextpage" href="../margin/">Margin-based Losses »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 26 August 2024 23:15">Monday 26 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
