<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Working with Losses · LossFunctions.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://JuliaML.github.io/LossFunctions.jl/user/interface/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LossFunctions.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LossFunctions.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../introduction/gettingstarted/">Getting Started</a></li><li><a class="tocitem" href="../../introduction/motivation/">Background and Motivation</a></li></ul></li><li><span class="tocitem">User&#39;s Guide</span><ul><li class="is-active"><a class="tocitem" href>Working with Losses</a><ul class="internal"><li><a class="tocitem" href="#Instantiating-a-Loss"><span>Instantiating a Loss</span></a></li><li><a class="tocitem" href="#Computing-the-Values"><span>Computing the Values</span></a></li><li><a class="tocitem" href="#Computing-the-1st-Derivatives"><span>Computing the 1st Derivatives</span></a></li><li><a class="tocitem" href="#Computing-the-2nd-Derivatives"><span>Computing the 2nd Derivatives</span></a></li><li><a class="tocitem" href="#Properties-of-a-Loss"><span>Properties of a Loss</span></a></li></ul></li><li><a class="tocitem" href="../aggregate/">Efficient Sum and Mean</a></li></ul></li><li><span class="tocitem">Available Losses</span><ul><li><a class="tocitem" href="../../losses/distance/">Distance-based Losses</a></li><li><a class="tocitem" href="../../losses/margin/">Margin-based Losses</a></li><li><a class="tocitem" href="../../losses/other/">Other Losses</a></li></ul></li><li><span class="tocitem">Advances Topics</span><ul><li><a class="tocitem" href="../../advanced/extend/">Altering existing Losses</a></li><li><a class="tocitem" href="../../advanced/developer/">Developer Documentation</a></li></ul></li><li><a class="tocitem" href="../../acknowledgements/">Acknowledgements</a></li><li><a class="tocitem" href="../../LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User&#39;s Guide</a></li><li class="is-active"><a href>Working with Losses</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Working with Losses</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaML/LossFunctions.jl/blob/master/docs/src/user/interface.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Working-with-Losses"><a class="docs-heading-anchor" href="#Working-with-Losses">Working with Losses</a><a id="Working-with-Losses-1"></a><a class="docs-heading-anchor-permalink" href="#Working-with-Losses" title="Permalink"></a></h1><p>Even though they are called loss &quot;functions&quot;, this package implements them as immutable types instead of true Julia functions. There are good reasons for that. For example it allows us to specify the properties of loss functions explicitly (e.g. <code>isconvex(myloss)</code>). It also makes for a more consistent API when it comes to computing the value or the derivative. Some loss functions even have additional parameters that need to be specified, such as the <span>$\epsilon$</span> in the case of the <span>$\epsilon$</span>-insensitive loss. Here, types allow for member variables to hide that information away from the method signatures.</p><p>In order to avoid potential confusions with true Julia functions, we will refer to &quot;loss functions&quot; as &quot;losses&quot; instead. The available losses share a common interface for the most part. This section will provide an overview of the basic functionality that is available for all the different types of losses. We will discuss how to create a loss, how to compute its value and derivative, and how to query its properties.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>Loss</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>SupervisedLoss</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>UnsupervisedLoss</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Instantiating-a-Loss"><a class="docs-heading-anchor" href="#Instantiating-a-Loss">Instantiating a Loss</a><a id="Instantiating-a-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Instantiating-a-Loss" title="Permalink"></a></h2><p>Losses are immutable types. As such, one has to instantiate one in order to work with it. For most losses, the constructors do not expect any parameters.</p><pre><code class="language-julia-repl hljs">julia&gt; L2DistLoss()
L2DistLoss()

julia&gt; HingeLoss()
L1HingeLoss()</code></pre><p>We just said that we need to instantiate a loss in order to work with it. One could be inclined to belief, that it would be more memory-efficient to &quot;pre-allocate&quot; a loss when using it in more than one place.</p><pre><code class="language-julia-repl hljs">julia&gt; loss = L2DistLoss()
L2DistLoss()

julia&gt; loss(3, 2)
1</code></pre><p>However, that is a common oversimplification. Because all losses are immutable types, they can live on the stack and thus do not come with a heap-allocation overhead.</p><p>Even more interesting in the example above, is that for such losses as <a href="../../losses/distance/#LossFunctions.L2DistLoss"><code>L2DistLoss</code></a>, which do not have any constructor parameters or member variables, there is no additional code executed at all. Such singletons are only used for dispatch and don&#39;t even produce any additional code, which you can observe for yourself in the code below. As such they are zero-cost abstractions.</p><pre><code class="language-julia-repl hljs">julia&gt; v1(loss,y,t) = loss(y,t)

julia&gt; v2(y,t) = L2DistLoss()(y,t)

julia&gt; @code_llvm v1(loss, 3, 2)
define i64 @julia_v1_70944(i64, i64) #0 {
top:
  %2 = sub i64 %1, %0
  %3 = mul i64 %2, %2
  ret i64 %3
}

julia&gt; @code_llvm v2(3, 2)
define i64 @julia_v2_70949(i64, i64) #0 {
top:
  %2 = sub i64 %1, %0
  %3 = mul i64 %2, %2
  ret i64 %3
}</code></pre><p>On the other hand, some types of losses are actually more comparable to whole families of losses instead of just a single one. For example, the immutable type <a href="../../losses/distance/#LossFunctions.L1EpsilonInsLoss"><code>L1EpsilonInsLoss</code></a> has a free parameter <span>$\epsilon$</span>. Each concrete <span>$\epsilon$</span> results in a different concrete loss of the same family of epsilon-insensitive losses.</p><pre><code class="language-julia-repl hljs">julia&gt; L1EpsilonInsLoss(0.5)
L1EpsilonInsLoss{Float64}(0.5)

julia&gt; L1EpsilonInsLoss(1)
L1EpsilonInsLoss{Float64}(1.0)</code></pre><p>For such losses that do have parameters, it can make a slight difference to pre-instantiate a loss. While they will live on the stack, the constructor usually performs some assertions and conversion for the given parameter. This can come at a slight overhead. At the very least it will not produce the same exact code when pre-instantiated. Still, the fact that they are immutable makes them very efficient abstractions with little to no performance overhead, and zero memory allocations on the heap.</p><h2 id="Computing-the-Values"><a class="docs-heading-anchor" href="#Computing-the-Values">Computing the Values</a><a id="Computing-the-Values-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-Values" title="Permalink"></a></h2><p>The first thing we may want to do is compute the loss for some observation (singular). In fact, all losses are implemented on single observations under the hood, and are functors.</p><pre><code class="language-julia-repl hljs">julia&gt; loss = L1DistLoss()
L1DistLoss()

julia&gt; loss.([2,5,-2], [1,2,3])
3-element Vector{Int64}:
 1
 3
 5</code></pre><h2 id="Computing-the-1st-Derivatives"><a class="docs-heading-anchor" href="#Computing-the-1st-Derivatives">Computing the 1st Derivatives</a><a id="Computing-the-1st-Derivatives-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-1st-Derivatives" title="Permalink"></a></h2><p>Maybe the more interesting aspect of loss functions are their derivatives. In fact, most of the popular learning algorithm in Supervised Learning, such as gradient descent, utilize the derivatives of the loss in one way or the other during the training process.</p><p>To compute the derivative of some loss we expose the function <a href="user/@ref"><code>deriv</code></a>. It may be interesting to note explicitly, that we always compute the derivative in respect to the predicted <code>output</code>, since we are interested in deducing in which direction the output should change.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>deriv</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Computing-the-2nd-Derivatives"><a class="docs-heading-anchor" href="#Computing-the-2nd-Derivatives">Computing the 2nd Derivatives</a><a id="Computing-the-2nd-Derivatives-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-2nd-Derivatives" title="Permalink"></a></h2><p>Additionally to the first derivative, we also provide the corresponding methods for the second derivative through the function <a href="user/@ref"><code>deriv2</code></a>. Note again, that we always compute the derivative in respect to the predicted <code>output</code>.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>deriv2</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Properties-of-a-Loss"><a class="docs-heading-anchor" href="#Properties-of-a-Loss">Properties of a Loss</a><a id="Properties-of-a-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Properties-of-a-Loss" title="Permalink"></a></h2><p>In some situations it can be quite useful to assert certain properties about a loss-function. One such scenario could be when implementing an algorithm that requires the loss to be strictly convex or Lipschitz continuous. Note that we will only skim over the defintions in most cases. A good treatment of all of the concepts involved can be found in either <sup class="footnote-reference"><a id="citeref-BOYD2004" href="#footnote-BOYD2004">[BOYD2004]</a></sup> or <sup class="footnote-reference"><a id="citeref-STEINWART2008" href="#footnote-STEINWART2008">[STEINWART2008]</a></sup>.</p><p>This package uses functions to represent individual properties of a loss. It follows a list of implemented property-functions defined in <a href="https://github.com/JuliaML/LearnBase.jl">LearnBase.jl</a>.</p><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isdistancebased</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>ismarginbased</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isminimizable</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isdifferentiable</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>istwicedifferentiable</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isconvex</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isstrictlyconvex</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isstronglyconvex</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isnemitski</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isunivfishercons</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isfishercons</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>islipschitzcont</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>islocallylipschitzcont</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isclipable</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>isclasscalibrated</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>issymmetric</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-BOYD2004"><a class="tag is-link" href="#citeref-BOYD2004">BOYD2004</a>Stephen Boyd and Lieven Vandenberghe. <a href="https://stanford.edu/~boyd/cvxbook/">&quot;Convex Optimization&quot;</a>. Cambridge University Press, 2004.</li><li class="footnote" id="footnote-STEINWART2008"><a class="tag is-link" href="#citeref-STEINWART2008">STEINWART2008</a>Steinwart, Ingo, and Andreas Christmann. <a href="https://www.springer.com/us/book/9780387772417">&quot;Support vector machines&quot;</a>. Springer Science &amp; Business Media, 2008.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../introduction/motivation/">« Background and Motivation</a><a class="docs-footer-nextpage" href="../aggregate/">Efficient Sum and Mean »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 17 October 2025 18:43">Friday 17 October 2025</span>. Using Julia version 1.12.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
